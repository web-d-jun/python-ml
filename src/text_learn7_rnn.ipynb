{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3494\n",
      "빈도수 상위 582번 단어:offer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#https://www.kaggle.com/aashita/nyt-comments\n",
    "\n",
    "df = pd.read_csv('../file/ArticlesApril2018.csv')\n",
    "# df.head()\n",
    "\n",
    "headline = []\n",
    "headline.extend(list(df.headline.values))\n",
    "headline[:5]\n",
    "# print(len(headline))\n",
    "\n",
    "headline = [word for word in headline if word != 'Unknown']\n",
    "# print(len(headline))\n",
    "\n",
    "# headline[:5]\n",
    "\n",
    "def repreprocessing(raw_sentence):\n",
    "    preprocessed_sentence = raw_sentence.encode('utf8').decode('ascii','ignore')\n",
    "    return ''.join(word for word in preprocessed_sentence if word not in punctuation).lower()\n",
    "\n",
    "preprocessed_headline = [repreprocessing(x) for x in headline]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_headline)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "sequences = list()\n",
    "\n",
    "for sentence in preprocessed_headline:\n",
    "    encoded = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "index_to_word = {}\n",
    "for key, value in tokenizer.word_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "print('빈도수 상위 582번 단어:{}'.format(index_to_word[582]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  99]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99 269]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  99 269 371]]\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences)\n",
    "print(max_len)\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(y[:3])\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 23:41:04.124048: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 - 3s - loss: 7.6450 - accuracy: 0.0276 - 3s/epoch - 13ms/step\n",
      "Epoch 2/200\n",
      "244/244 - 3s - loss: 7.1076 - accuracy: 0.0288 - 3s/epoch - 11ms/step\n",
      "Epoch 3/200\n",
      "244/244 - 3s - loss: 6.9748 - accuracy: 0.0364 - 3s/epoch - 11ms/step\n",
      "Epoch 4/200\n",
      "244/244 - 3s - loss: 6.8548 - accuracy: 0.0432 - 3s/epoch - 12ms/step\n",
      "Epoch 5/200\n",
      "244/244 - 3s - loss: 6.7141 - accuracy: 0.0436 - 3s/epoch - 11ms/step\n",
      "Epoch 6/200\n",
      "244/244 - 3s - loss: 6.5642 - accuracy: 0.0482 - 3s/epoch - 11ms/step\n",
      "Epoch 7/200\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "embedding_dim = 10\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to be rich and im not sorry with say\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n):\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "\n",
    "    for _ in range(n):\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
    "        result = model.predict([encoded], verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "             if index == result:\n",
    "                break\n",
    "        \n",
    "        current_word = current_word+ ' ' + word\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence\n",
    "\n",
    "\n",
    "print(sentence_generation(model, tokenizer, 'i', 10))\n",
    "# i want to be rich and im not sorry with say "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to make facebook more accountable to live in a bleached'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(model, tokenizer, 'how', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "\n",
    "f = open('11-0.txt', 'rb')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for sentence in f:\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.decode('ascii','ignore')\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "f.close()\n",
    "\n",
    "total_data = ' '.join(sentences)\n",
    "\n",
    "char_vocab = sorted(list(set(total_data)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/junyoung/dev/python-ml/src/text_learn7_rnn.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/junyoung/dev/python-ml/src/text_learn7_rnn.ipynb#ch0000000?line=0'>1</a>\u001b[0m char_to_index \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((char, index) \u001b[39mfor\u001b[39;00m index, char \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(char_vocab))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junyoung/dev/python-ml/src/text_learn7_rnn.ipynb#ch0000000?line=1'>2</a>\u001b[0m index_to_char \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/junyoung/dev/python-ml/src/text_learn7_rnn.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m char_to_index\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((char, index) for index, char in enumerate(char_vocab))\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "\n",
    "print(index_to_char)\n",
    "train_X = 'appl'\n",
    "train_y = 'pple'\n",
    "\n",
    "seq_length = 60\n",
    "\n",
    "n_samples = int(np.floor((len(total_data) - 1) / seq_length))\n",
    "print ('샘플의 수 : {}'.format(n_samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b20a575b3bea1b2146886c516952439d9e7d15a399f16c2c9174de2a329083a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
