{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junyoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/junyoung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/junyoung/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                  need troubleshoot help\n",
      "1         chamber dsttn pn request fuse cap heater driver\n",
      "2       plea tse search amat p n match model number n c f\n",
      "3       communic ffu fi server follow error seen fi gu...\n",
      "4       z provis support need third parti netapp part ...\n",
      "                              ...                        \n",
      "8559    ch sinc day rf coil reflect power pm ad detail...\n",
      "8560             pump slow random communic cabl hook pump\n",
      "8561    fi robot fault transfer requir initi wafer loc...\n",
      "8562    unscrew viewport window chamber bodi helicoil ...\n",
      "8563                                        procedur need\n",
      "Name: escalationtext1, Length: 8564, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy\n",
    "import nltk\n",
    "import string\n",
    "from io import StringIO\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import punkt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas.testing as tm\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import preprocessing\n",
    "from elasticsearch import Elasticsearch\n",
    "from pandasticsearch import DataFrame\n",
    "\n",
    "\n",
    "data = pd.read_csv('../file/data_dropna.csv')\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def data_text_cleaning(x):\n",
    "    only_english = re.sub('[^a-zA-Z]', ' ', x)\n",
    "    no_capitals = only_english.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    no_stops = [word for word in no_capitals if not word in stops]\n",
    "    stemmer = nltk.stem.SnowballStemmer('english') #형태소 분석기\n",
    "    stemmer_words = [stemmer.stem(word) for word in no_stops] #어간 추출\n",
    "    wordnet_lemmatizer = WordNetLemmatizer() #표제어추출\n",
    "    words = [wordnet_lemmatizer.lemmatize(w) for w in stemmer_words]\n",
    "    \n",
    "\n",
    "    return words\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "data['resolutiontext1'] = data['resolutiontext'].apply(lambda x: data_text_cleaning(x))\n",
    "data['resolutiontext1'] = data['resolutiontext1'].apply(lambda x: \" \".join(str(c) for c in x))\n",
    "\n",
    "data['servicetext1'] = data['servicetext'].apply(lambda x: data_text_cleaning(str(x)))\n",
    "data['servicetext1'] = data['servicetext1'].apply(lambda x: \" \".join(str(c) for c in x))\n",
    "\n",
    "data['escalationtext1'] = data['escalationtext'].apply(lambda x: data_text_cleaning(str(x)))\n",
    "data['escalationtext1'] = data['escalationtext1'].apply(lambda x: \" \".join(str(c) for c in x))\n",
    "\n",
    "print(data['escalationtext1'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b20a575b3bea1b2146886c516952439d9e7d15a399f16c2c9174de2a329083a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
