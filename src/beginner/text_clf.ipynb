{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>z13th1q4yzihf1bll23qxzpjeujterydj</td>\n",
       "      <td>Carmen Racasanu</td>\n",
       "      <td>2014-11-14T13:27:52</td>\n",
       "      <td>How can this have 2 billion views when there's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k</td>\n",
       "      <td>diego mogrovejo</td>\n",
       "      <td>2014-11-14T13:28:08</td>\n",
       "      <td>I don't now why I'm watching this in 2014﻿</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>z130zd5b3titudkoe04ccbeohojxuzppvbg</td>\n",
       "      <td>BlueYetiPlayz -Call Of Duty and More</td>\n",
       "      <td>2015-05-23T13:04:32</td>\n",
       "      <td>subscribe to me for call of duty vids and give...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>z12he50arvrkivl5u04cctawgxzkjfsjcc4</td>\n",
       "      <td>Photo Editor</td>\n",
       "      <td>2015-06-05T14:14:48</td>\n",
       "      <td>hi guys please my android photo editor downloa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k</td>\n",
       "      <td>Ray Benich</td>\n",
       "      <td>2015-06-05T18:05:16</td>\n",
       "      <td>The first billion viewed this because they tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                COMMENT_ID  \\\n",
       "345      z13th1q4yzihf1bll23qxzpjeujterydj   \n",
       "346  z13fcn1wfpb5e51xe04chdxakpzgchyaxzo0k   \n",
       "347    z130zd5b3titudkoe04ccbeohojxuzppvbg   \n",
       "348    z12he50arvrkivl5u04cctawgxzkjfsjcc4   \n",
       "349  z13vhvu54u3ewpp5h04ccb4zuoardrmjlyk0k   \n",
       "\n",
       "                                   AUTHOR                 DATE  \\\n",
       "345                       Carmen Racasanu  2014-11-14T13:27:52   \n",
       "346                       diego mogrovejo  2014-11-14T13:28:08   \n",
       "347  BlueYetiPlayz -Call Of Duty and More  2015-05-23T13:04:32   \n",
       "348                          Photo Editor  2015-06-05T14:14:48   \n",
       "349                            Ray Benich  2015-06-05T18:05:16   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "345  How can this have 2 billion views when there's...      0  \n",
       "346         I don't now why I'm watching this in 2014﻿      0  \n",
       "347  subscribe to me for call of duty vids and give...      1  \n",
       "348  hi guys please my android photo editor downloa...      1  \n",
       "349  The first billion viewed this because they tho...      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv('./input/Youtube01-Psy.csv')\n",
    "d.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.query('CLASS == 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "dvec = vectorizer.fit_transform(d['CONTENT'])\n",
    "# vectorizer.get_feature_names()\n",
    "\n",
    "dshuf = d.sample(frac=1)\n",
    "len(dshuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "(50,)\n",
      "300\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "d_train= dshuf[:300]\n",
    "d_test= dshuf[300:]\n",
    "d_train_att = vectorizer.fit_transform(d_train['CONTENT'])\n",
    "d_test_att = vectorizer.transform(d_test['CONTENT'])\n",
    "d_train_label = d_train['CLASS']\n",
    "d_test_label = d_test['CLASS']\n",
    "print(len(d_test_label))\n",
    "print(d_test_label.shape)\n",
    "\n",
    "print(len(d_train_label))\n",
    "print(d_train_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=80)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=80)\n",
    "clf.fit(d_train_att, d_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(d_test_att, d_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  0],\n",
       "       [ 1, 24]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred_labels = clf.predict(d_test_att)\n",
    "confusion_matrix(d_test_label, pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.029814239699997188\n",
      "[0.95       0.93333333 0.93333333 0.96666667 0.96666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, d_train_att, d_train_label, cv=5)\n",
    "print(scores.mean(), scores.std() * 2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1956"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.concat([\n",
    "    pd.read_csv('./input/Youtube01-Psy.csv'),\n",
    "    pd.read_csv('./input/Youtube02-KatyPerry.csv'),\n",
    "    pd.read_csv('./input/Youtube03-LMFAO.csv'),\n",
    "    pd.read_csv('./input/Youtube04-Eminem.csv'),\n",
    "    pd.read_csv('./input/Youtube05-Shakira.csv')\n",
    "])\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshuf = d.sample(frac=1)\n",
    "d_content = dshuf['CONTENT']\n",
    "d_label = dshuf['CLASS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bag-of-words', CountVectorizer()),\n",
       "                ('random forest', RandomForestClassifier())])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "pipline = Pipeline([\n",
    "    ('bag-of-words', CountVectorizer()),\n",
    "    ('random forest', RandomForestClassifier()),\n",
    "])\n",
    "pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('randomforestclassifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pipeline(CountVectorizer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bag-of-words', CountVectorizer()),\n",
       "                ('random forest', RandomForestClassifier())])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.fit(d_content[:1500], d_label[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.score(d_content[1500:], d_label[1500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.predict(['what a neat video!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline.predict(['plz subscribe to my channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9580862779894567 0.026914147165359514\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipline, d_content, d_label, cv=5)\n",
    "print(scores.mean(), scores.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipline2 = make_pipeline(CountVectorizer(), TfidfTransformer(norm=None), RandomForestClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585925674617674 0.023610496432332404\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipline2, d_content, d_label, cv=5)\n",
    "print(scores.mean(), scores.std() * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', CountVectorizer()),\n",
       " ('tfidftransformer', TfidfTransformer(norm=None)),\n",
       " ('randomforestclassifier', RandomForestClassifier())]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline2.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'countvectorizer__max_features': (None, 1000, 2000),\n",
    "    'countvectorizer__ngram_range' : ((1,1), (1,2)),\n",
    "    'countvectorizer__stop_words' : ('english', None),\n",
    "    'tfidftransformer__use_idf' : (True, False),\n",
    "    'randomforestclassifier__n_estimators': (20,50,100)\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(pipline2, parameters, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                                       ('tfidftransformer',\n",
       "                                        TfidfTransformer(norm=None)),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'countvectorizer__max_features': (None, 1000, 2000),\n",
       "                         'countvectorizer__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'countvectorizer__stop_words': ('english', None),\n",
       "                         'randomforestclassifier__n_estimators': (20, 50, 100),\n",
       "                         'tfidftransformer__use_idf': (True, False)},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(d_content, d_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.963\n",
      "countvectorizer__max_features: 2000\n",
      "countvectorizer__ngram_range: (1, 1)\n",
      "countvectorizer__stop_words: 'english'\n",
      "randomforestclassifier__n_estimators: 50\n",
      "tfidftransformer__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "print('best score %0.3f' % grid_search.best_score_)\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s :%(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 14:29:02,737 : INFO :loading projection weights from ./input/source/GoogleNews-vectors-negative300.bin\n",
      "2022-08-16 14:29:32,803 : INFO :KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from ./input/source/GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-08-16T14:29:32.803504', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "gmodel = gensim.models.keyedvectors.load_word2vec_format('./input/source/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76094574"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.similarity('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12412613"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.similarity('cat','spatula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana', 'good', 'hahahaha', 'tomato', 'beginner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent)\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent)\n",
    "    sent = re.sub(r'\\W', ' ', sent)\n",
    "    sent = re.sub(r'\\s+', ' ', sent)\n",
    "    sent = sent.strip()\n",
    "    return sent.split()\n",
    "\n",
    "a = extract_words('apple banana,!! good hahahaha tomato. beginner')\n",
    "\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "unsup_sentences = []\n",
    "\n",
    "for dirname in ['train/pos', 'test/pos']:\n",
    "    for fname in sorted(os.listdir('./input/sources/aclImdb/' + dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open('./input/sources/aclImdb/' + dirname + '/' + fname, encoding='UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words, [dirname + '/' + fname]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unsup_sentences)\n",
    "\n",
    "import random\n",
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 15:20:18,521 : INFO :collecting all words and their counts\n",
      "2022-08-16 15:20:18,541 : INFO :PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2022-08-16 15:20:19,088 : INFO :PROGRESS: at example #10000, processed 2330982 words (4273153 words/s), 54938 word types, 10000 tags\n",
      "2022-08-16 15:20:19,527 : INFO :PROGRESS: at example #20000, processed 4655961 words (5302468 words/s), 73335 word types, 20000 tags\n",
      "2022-08-16 15:20:19,880 : INFO :collected 80269 word types and 25000 unique tags from a corpus of 25000 examples and 5818646 words\n",
      "2022-08-16 15:20:19,880 : INFO :Creating a fresh vocabulary\n",
      "2022-08-16 15:20:20,005 : INFO :Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 30366 unique words (37.83% of original 80269, drops 49903)', 'datetime': '2022-08-16T15:20:20.005763', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-16 15:20:20,006 : INFO :Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5735524 word corpus (98.57% of original 5818646, drops 83122)', 'datetime': '2022-08-16T15:20:20.006802', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-16 15:20:20,146 : INFO :deleting the raw counts dictionary of 80269 items\n",
      "2022-08-16 15:20:20,151 : INFO :sample=0.001 downsamples 43 most-common words\n",
      "2022-08-16 15:20:20,151 : INFO :Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4314455.11372325 word corpus (75.2%% of prior 5735524)', 'datetime': '2022-08-16T15:20:20.151101', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-08-16 15:20:20,169 : INFO :constructing a huffman tree from 30366 words\n",
      "2022-08-16 15:20:20,903 : INFO :built huffman tree with maximum node depth 20\n",
      "2022-08-16 15:20:21,086 : INFO :estimated required memory for 30366 words and 100 dimensions: 72695400 bytes\n",
      "2022-08-16 15:20:21,088 : INFO :resetting layer weights\n",
      "2022-08-16 15:20:21,111 : INFO :Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 30366 vocabulary and 100 features, using sg=1 hs=1 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-08-16T15:20:21.111141', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "2022-08-16 15:20:22,139 : INFO :EPOCH 0 - PROGRESS: at 16.54% examples, 721222 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:23,139 : INFO :EPOCH 0 - PROGRESS: at 35.14% examples, 760076 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:24,147 : INFO :EPOCH 0 - PROGRESS: at 51.20% examples, 736541 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:25,163 : INFO :EPOCH 0 - PROGRESS: at 68.05% examples, 732939 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:26,164 : INFO :EPOCH 0 - PROGRESS: at 86.46% examples, 742968 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:26,883 : INFO :EPOCH 0: training on 5818646 raw words (4338334 effective words) took 5.8s, 754249 effective words/s\n",
      "2022-08-16 15:20:27,896 : INFO :EPOCH 1 - PROGRESS: at 18.53% examples, 795524 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:28,901 : INFO :EPOCH 1 - PROGRESS: at 37.67% examples, 809607 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:29,901 : INFO :EPOCH 1 - PROGRESS: at 56.62% examples, 812274 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:30,911 : INFO :EPOCH 1 - PROGRESS: at 76.18% examples, 821175 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:31,923 : INFO :EPOCH 1 - PROGRESS: at 92.89% examples, 800071 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:32,369 : INFO :EPOCH 1: training on 5818646 raw words (4338980 effective words) took 5.5s, 792083 effective words/s\n",
      "2022-08-16 15:20:33,389 : INFO :EPOCH 2 - PROGRESS: at 18.43% examples, 800111 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:34,398 : INFO :EPOCH 2 - PROGRESS: at 35.19% examples, 763468 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:35,401 : INFO :EPOCH 2 - PROGRESS: at 51.28% examples, 736530 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:36,413 : INFO :EPOCH 2 - PROGRESS: at 69.32% examples, 748247 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:37,424 : INFO :EPOCH 2 - PROGRESS: at 88.87% examples, 764218 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:38,027 : INFO :EPOCH 2: training on 5818646 raw words (4340063 effective words) took 5.6s, 768984 effective words/s\n",
      "2022-08-16 15:20:39,040 : INFO :EPOCH 3 - PROGRESS: at 16.50% examples, 717484 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:40,042 : INFO :EPOCH 3 - PROGRESS: at 33.19% examples, 720167 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:41,045 : INFO :EPOCH 3 - PROGRESS: at 52.60% examples, 760184 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:42,049 : INFO :EPOCH 3 - PROGRESS: at 70.87% examples, 770383 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:43,053 : INFO :EPOCH 3 - PROGRESS: at 90.20% examples, 781041 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:43,592 : INFO :EPOCH 3: training on 5818646 raw words (4339307 effective words) took 5.6s, 781633 effective words/s\n",
      "2022-08-16 15:20:44,619 : INFO :EPOCH 4 - PROGRESS: at 17.90% examples, 759167 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:45,619 : INFO :EPOCH 4 - PROGRESS: at 36.05% examples, 774731 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:46,628 : INFO :EPOCH 4 - PROGRESS: at 54.28% examples, 780456 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:47,629 : INFO :EPOCH 4 - PROGRESS: at 71.73% examples, 777686 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:48,636 : INFO :EPOCH 4 - PROGRESS: at 89.76% examples, 777966 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:49,147 : INFO :EPOCH 4: training on 5818646 raw words (4339882 effective words) took 5.5s, 783632 effective words/s\n",
      "2022-08-16 15:20:50,166 : INFO :EPOCH 5 - PROGRESS: at 17.29% examples, 730609 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:51,172 : INFO :EPOCH 5 - PROGRESS: at 35.00% examples, 746504 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:52,176 : INFO :EPOCH 5 - PROGRESS: at 49.28% examples, 704567 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:53,183 : INFO :EPOCH 5 - PROGRESS: at 64.32% examples, 692917 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:54,201 : INFO :EPOCH 5 - PROGRESS: at 80.73% examples, 695306 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:55,201 : INFO :EPOCH 5 - PROGRESS: at 99.75% examples, 715769 words/s, in_qsize 2, out_qsize 1\n",
      "2022-08-16 15:20:55,216 : INFO :EPOCH 5: training on 5818646 raw words (4339424 effective words) took 6.1s, 716002 effective words/s\n",
      "2022-08-16 15:20:56,244 : INFO :EPOCH 6 - PROGRESS: at 15.33% examples, 657034 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:57,244 : INFO :EPOCH 6 - PROGRESS: at 31.03% examples, 666107 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:20:58,246 : INFO :EPOCH 6 - PROGRESS: at 44.88% examples, 644195 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:20:59,251 : INFO :EPOCH 6 - PROGRESS: at 60.09% examples, 647446 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:21:00,258 : INFO :EPOCH 6 - PROGRESS: at 75.30% examples, 649872 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:01,264 : INFO :EPOCH 6 - PROGRESS: at 89.64% examples, 645013 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:21:02,020 : INFO :EPOCH 6: training on 5818646 raw words (4339917 effective words) took 6.8s, 639378 effective words/s\n",
      "2022-08-16 15:21:03,047 : INFO :EPOCH 7 - PROGRESS: at 17.62% examples, 758136 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:04,052 : INFO :EPOCH 7 - PROGRESS: at 33.88% examples, 731804 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:05,053 : INFO :EPOCH 7 - PROGRESS: at 50.84% examples, 731359 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:06,055 : INFO :EPOCH 7 - PROGRESS: at 68.10% examples, 731856 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:07,066 : INFO :EPOCH 7 - PROGRESS: at 83.90% examples, 722989 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:07,988 : INFO :EPOCH 7: training on 5818646 raw words (4339152 effective words) took 6.0s, 729133 effective words/s\n",
      "2022-08-16 15:21:09,011 : INFO :EPOCH 8 - PROGRESS: at 16.63% examples, 713411 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:10,015 : INFO :EPOCH 8 - PROGRESS: at 30.51% examples, 666280 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:11,017 : INFO :EPOCH 8 - PROGRESS: at 47.23% examples, 683559 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:12,018 : INFO :EPOCH 8 - PROGRESS: at 63.44% examples, 687914 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:13,026 : INFO :EPOCH 8 - PROGRESS: at 79.92% examples, 691767 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:14,034 : INFO :EPOCH 8 - PROGRESS: at 96.23% examples, 692751 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:14,238 : INFO :EPOCH 8: training on 5818646 raw words (4339046 effective words) took 6.2s, 696099 effective words/s\n",
      "2022-08-16 15:21:15,254 : INFO :EPOCH 9 - PROGRESS: at 16.88% examples, 738692 words/s, in_qsize 6, out_qsize 0\n",
      "2022-08-16 15:21:16,261 : INFO :EPOCH 9 - PROGRESS: at 34.26% examples, 747517 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:17,264 : INFO :EPOCH 9 - PROGRESS: at 52.14% examples, 753044 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:18,276 : INFO :EPOCH 9 - PROGRESS: at 69.64% examples, 752784 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:19,279 : INFO :EPOCH 9 - PROGRESS: at 82.20% examples, 711418 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:20,283 : INFO :EPOCH 9 - PROGRESS: at 96.93% examples, 698480 words/s, in_qsize 5, out_qsize 0\n",
      "2022-08-16 15:21:20,450 : INFO :EPOCH 9: training on 5818646 raw words (4339883 effective words) took 6.2s, 700090 effective words/s\n",
      "2022-08-16 15:21:20,451 : INFO :Doc2Vec lifecycle event {'msg': 'training on 58186460 raw words (43393988 effective words) took 59.3s, 731284 effective words/s', 'datetime': '2022-08-16T15:21:20.451430', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "2022-08-16 15:21:20,452 : INFO :Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow,d100,n5,hs,mc5,s0.001,t3>', 'datetime': '2022-08-16T15:21:20.452415', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "permuter = PermuteSentences(unsup_sentences)\n",
    "model = Doc2Vec(permuter, dm=0, hs=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sklearn1.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9720f2c0a5fb07196a9441b0d7f13d8f98b4e4557b919c0543106eefd2c02f5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
