{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from hanspell import spell_checker\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "tokens = okt.morphs('나는 자연어 처리를 배운다')\n",
    "\n",
    "word_to_index = {word : index for index, word in enumerate(tokens) }\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(word, word_to_index):\n",
    "    \n",
    "    one_hot_vector = [0]*(len(word_to_index))\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    \n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "one_hot_encoding('자연어', word_to_index)\n",
    "\n",
    "\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "# print(tokenizer.word_index)\n",
    "\n",
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])\n",
    "# print(encoded)\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "# print(one_hot)\n",
    "\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
    "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
    "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
    "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
    "columns = ['메일 본문', '스팸 메일 유무']\n",
    "\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "df\n",
    "\n",
    "X = df['메일 본문']\n",
    "y = df['스팸 메일 유무']\n",
    "\n",
    "# print(X.to_list())\n",
    "# print(y.to_list())\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sent = \"김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "\n",
    "\n",
    "# 마지막 열을 제외하고 X데이터에 저장합니다. 마지막 열만을 y데이터에 저장합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\n",
    "\n",
    "\n",
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.625 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "\n",
    "for document in corpus:\n",
    "  if len(document) > i:\n",
    "    i = i + 1\n",
    "  if i == 3:\n",
    "    break\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08838002913645132"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table[\"반포한\"].cohesion_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores =  {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "\n",
    "l_tokenizer = LTokenizer(scores=scores)\n",
    "l_tokenizer.tokenize(\"국제사회와 우리의 노력들로 범죄를 척결하자\", flatten=False)\n",
    "\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"국제사회와우리의노력들로범죄를척결하자\") \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋㅋ영화존잼쓰ㅠㅠ\n",
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "emo = emoticon_normalize(\"앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ\", num_repeats=2)\n",
    "print(emo)\n",
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "  # 온점 제거 및 형태소 분석\n",
    "  document = document.replace('.', '')\n",
    "  tokenized_document = okt.morphs(document)\n",
    "\n",
    "  word_to_index = {}\n",
    "  bow = []\n",
    "\n",
    "  for word in tokenized_document:  \n",
    "    if word not in word_to_index.keys():\n",
    "      word_to_index[word] = len(word_to_index)  \n",
    "      # BoW에 전부 기본값 1을 넣는다.\n",
    "      bow.insert(len(word_to_index) - 1, 1)\n",
    "    else:\n",
    "      # 재등장하는 단어의 인덱스\n",
    "      index = word_to_index.get(word)\n",
    "      print(index)\n",
    "      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
    "      bow[index] = bow[index] + 1\n",
    "\n",
    "  return word_to_index, bow\n",
    "\n",
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary :', vocab)\n",
    "print('bag of words vector :', bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector : [[1 1 2 1 1 1]]\n",
      "vocabulary : {'정부가': 5, '발표하는': 3, '물가상승률': 2, '소비자가': 4, '느끼는': 0, '다르다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['정부가 발표하는 물가상승률 소비자가 느끼는 물가상승률 다르다.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print('bag of words vector :', vector.fit_transform(corpus).toarray()) \n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print('vocabulary :',vector.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "]\n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()\n",
    "print(vocab)\n",
    "\n",
    "N = len(docs) #문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        \n",
    "        df += t in doc\n",
    "        \n",
    "    return log(N/(df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        tf(t,d)\n",
    "        result[-1].append(tf(t,d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns=vocab)\n",
    "\n",
    "tf_\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6931471805599453, 0.6931471805599453, 0.6931471805599453, 0.28768207245178085, 0.28768207245178085, 0.6931471805599453, 0.28768207245178085, 0.6931471805599453, 0.6931471805599453]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    # print(t)\n",
    "    result.append(idf(t))\n",
    "print(result)\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t, d))\n",
    "    \n",
    "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>0.467351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3        4         5        6  \\\n",
       "0  0.00000  0.467351  0.000000  0.467351  0.00000  0.467351  0.00000   \n",
       "1  0.00000  0.000000  0.795961  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.57735  0.000000  0.000000  0.000000  0.57735  0.000000  0.57735   \n",
       "\n",
       "          7         8  \n",
       "0  0.355432  0.467351  \n",
       "1  0.605349  0.000000  \n",
       "2  0.000000  0.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "# print(tfidf.transform(corpus).toarray())\n",
    "print(tfidf.vocabulary_)\n",
    "test = tfidf.transform(corpus).toarray()\n",
    "test_ = pd.DataFrame(test)\n",
    "\n",
    "test_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [1 0 1 1]\n",
      " [1 0 1 1]]\n",
      "문서 1과 문서2의 유사도 : 0.6666666666666667\n",
      "문서 1과 문서3의 유사도 : 0.6666666666666667\n",
      "문서 2와 문서3의 유사도 : 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    '저는 사과 좋아요',\n",
    "    '저는 바나나 좋아요',\n",
    "    '저는 바나나 좋아요'\n",
    "]\n",
    "\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "docs = vector.fit_transform(corpus).toarray()\n",
    "\n",
    "print(docs)\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print('문서 1과 문서2의 유사도 :',cos_sim(docs[0], docs[1]))\n",
    "print('문서 1과 문서3의 유사도 :',cos_sim(docs[0], docs[2]))\n",
    "print('문서 2와 문서3의 유사도 :',cos_sim(docs[1], docs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "0\n",
      "(20000, 47487)\n",
      "(20000, 20000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = pd.read_csv('../file/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "data = data.head(20000)\n",
    "\n",
    "\n",
    "\n",
    "print(data['overview'].isnull().sum())\n",
    "data['overview'] = data['overview'].fillna('')\n",
    "print(data['overview'].isnull().sum())\n",
    "\n",
    "# vector = CountVectorizer()\n",
    "\n",
    "# print(vector.fit_transform(data['overview']).toarray())\n",
    "# print(vector.vocabulary_)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])\n",
    "# print(tfidf.vocabulary_)\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "# Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences.\n",
    "\n",
    "\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12481                            The Dark Knight\n",
       "150                               Batman Forever\n",
       "1328                              Batman Returns\n",
       "15511                 Batman: Under the Red Hood\n",
       "585                                       Batman\n",
       "9230          Batman Beyond: Return of the Joker\n",
       "18035                           Batman: Year One\n",
       "19792    Batman: The Dark Knight Returns, Part 1\n",
       "3095                Batman: Mask of the Phantasm\n",
       "10122                              Batman Begins\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_to_index = dict(zip(data['title'], data.index))\n",
    "\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = title_to_index[title]\n",
    "    print(idx)\n",
    "    sim_scroes = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    sim_scores = sorted(sim_scroes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    movie_indices = [idx[0] for idx in sim_scores]\n",
    "\n",
    "\n",
    "    return data['title'].iloc[movie_indices]\n",
    "    \n",
    "\n",
    "\n",
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15348               Toy Story 3\n",
       "2997                Toy Story 2\n",
       "10301    The 40 Year Old Virgin\n",
       "8327                  The Champ\n",
       "1071      Rebel Without a Cause\n",
       "11399    For Your Consideration\n",
       "1932                  Condorman\n",
       "3057            Man on the Moon\n",
       "485                      Malice\n",
       "11606              Factory Girl\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = pd.read_csv('../file/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "data = data.head(20000)\n",
    "data['overview'] = data['overview'].fillna('')\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "title_to_index = dict(zip(data['title'], data.index))\n",
    "\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = title_to_index[title]\n",
    "    \n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    movie_indices = [idx[0] for idx in sim_scores]\n",
    "    \n",
    "    return data['title'].iloc[movie_indices]\n",
    "\n",
    "\n",
    "get_recommendations('Toy Story')\n",
    "# 여러가지 유사도 기법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a570dedfbda585c6ec6045193aed6d5d46c787d1787ff4dc0a01ac23261a8ae5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
